{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.7\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "1. using all publications of researchers with different weight as input to generate user profiles\n",
    "2. pretrain word2vec model window_5.model.bin and candidate_paper.csv are available via google drive link,\n",
    "you can download the files and\n",
    "change the path in this script so as to run the script successfully.\n",
    "3. result saved in rank_result_all_weight/weight_CP.csv\n",
    "\"\"\"\n",
    "import sys\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# define DocSim class to calculate document similarities\n",
    "class DocSim(object):\n",
    "    def __init__(self, w2v_model , stopwords=[]):\n",
    "        self.w2v_model = w2v_model\n",
    "        self.stopwords = stopwords\n",
    "\n",
    "    def vectorize(self, doc):\n",
    "        \"\"\"Identify the vector values for each word in the given document\"\"\"\n",
    "        doc = str(doc)\n",
    "        doc = doc.lower()\n",
    "        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                vec = self.w2v_model[word]\n",
    "                word_vecs.append(vec)\n",
    "            except KeyError:\n",
    "                # Ignore, if the word doesn't exist in the vocabulary\n",
    "                pass\n",
    "\n",
    "        # Assuming that document vector is the mean of all the word vectors\n",
    "        vector = np.mean(word_vecs, axis=0)\n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-train model on my own corpus\n",
    "model = 'window_5.model.bin'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "ds = DocSim(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert a text file to w2v vector and save the vector to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_file_name(file_path):\n",
    "    # input a cadidate paper file path, return the paperID as file name\n",
    "    # file_path = \"/AAMAS/AAMAS2005/p100-pechoucek.pdf\\n\"\n",
    "    # return \"AAMAS05-p100-pechoucek\"\n",
    "    paperName = file_path.split(\"/\")[-1].replace(\".pdf\\n\", \"\")\n",
    "    confName = file_path.split(\"/\")[-3]\n",
    "    year = file_path.split(\"/\")[-2][-2:]\n",
    "    name = f\"{confName}{year}-{paperName}\"\n",
    "    return name\n",
    "\n",
    "def userfile2vec(file_path, name):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.readline()\n",
    "    vec = ds.vectorize(text)\n",
    "    # save the vector to a local file\n",
    "    np.savetxt(f\"{name}.csv\",vec, delimiter=',' )\n",
    "    # load vector\n",
    "    vec = np.loadtxt(f\"{name}.csv\",delimiter=',')\n",
    "    return vec\n",
    "\n",
    "def paper2vec(paperID, text):\n",
    "    name = get_file_name(paperID)\n",
    "    vec = ds.vectorize(text)\n",
    "    np.savetxt(f\"candidate_vectors/{name}.csv\",vec, delimiter=',' )\n",
    "    # load vector\n",
    "    vec = np.loadtxt(f\"candidate_vectors/{name}.csv\",delimiter=',')\n",
    "    return vec\n",
    "\n",
    "def iter_files(rootDir):\n",
    "    \"\"\"Traverse the root directory to return a list of txt file paths for all leaf nodes\"\"\"\n",
    "    file_name_list = []\n",
    "    for root,dirs,files in os.walk(rootDir):\n",
    "        for file in files:\n",
    "            file_name = os.path.join(root,file)\n",
    "            file_name_list.append(file_name)\n",
    "    return file_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherry/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/sherry/Library/Python/3.9/lib/python/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# read all candidate papers info, contain two columns: paper ID and paper content\n",
    "\n",
    "candidate_papers = pd.read_csv('candidate_papers.csv')\n",
    "# rename columns in user_profile and candidate_papers\n",
    "candidate_papers.columns = ['paperID', 'paperText']\n",
    "fail_ls = []\n",
    "for i in range(len(candidate_papers['paperID'])):\n",
    "    paperID = candidate_papers['paperID'][i]\n",
    "    text = candidate_papers['paperText'][i]\n",
    "    try:\n",
    "        vec = paper2vec(paperID, text)\n",
    "    except:\n",
    "        fail_ls.append(paperID)\n",
    "pd.DataFrame(fail_ls).to_csv(\"fail_candidate_vectors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
